{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef\n",
    "from rdflib.namespace import OWL, RDF, RDFS,XSD, Namespace\n",
    "import csv\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import numpy as np\n",
    "#import tensorflow as tf pytorch tensor != tf tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"https://dbpedia.org/ontology/\"\n",
    "possible_types = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(relation):\n",
    "    r_split = relation.split(\"/\")\n",
    "    return r_split[len(r_split)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_type(property):\n",
    "    split_p = property.split(\"^^\")\n",
    "    p_type = str(split_p[1].split(\"#\")[1]).lower()\n",
    "    \n",
    "    if p_type.startswith(\"xsd:integer\"):\n",
    "        return(\"Integer\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:string\"):\n",
    "        return(\"String\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:double\"):\n",
    "        return(\"Double\", split_p[0])\n",
    "    if p_type.startswith(\"xsd:gYear\"):\n",
    "        return(\"Year\",split_p[0])\n",
    "    if p_type.startswith(\"xsd:date\"):\n",
    "        return(\"Date\",split_p[0])\n",
    "    return (\"\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N05c8df145ad74fb0a4e4dd00d3d223f1 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology = rdflib.Graph()\n",
    "ontology.parse('/home/sara/Desktop/fase2/git_repo/knowledge-graph-learning/data/external/ontologia.ttl', format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology.bind(\"dbo\", Namespace(\"http://dbpedia.org/ontology/\"))\n",
    "ontology.bind(\"dbr\", Namespace(\"http://dbpedia.org/resource/\"))\n",
    "ontology.bind(\"rdfs\", Namespace(\"http://www.w3.org/2000/01/rdf-schema#\"))\n",
    "ontology.bind(\"owl\", Namespace(\"http://www.w3.org/2002/07/owl#\"))\n",
    "ontology.bind(\"rdf\", Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_and_type = {}\n",
    "properties_and_types ={}\n",
    "relations = []\n",
    "triples = []\n",
    "use_properties = True\n",
    "# Process the Knowledge Graph\n",
    "g = rdflib.Graph()\n",
    "g.parse('/home/sara/Desktop/fase2/git_repo/knowledge-graph-learning/data/external/prova_c3.nt', format='nt')\n",
    "for s, p, o in g:\n",
    "    str_s = str(s)\n",
    "    str_p = str(p)\n",
    "    str_o = str(o)\n",
    "    if str_p != str(RDF.type):\n",
    "        if not str_s in entities_and_type.keys():\n",
    "            entities_and_type[(str_s)] =[]\n",
    "        if not str_p in relations:\n",
    "            relations.append(str_p)\n",
    "\n",
    "        if str_o.find('^^') == -1:\n",
    "            if not str_o in entities_and_type.keys():\n",
    "                entities_and_type[str_o]=[]\n",
    "            triples.append((str_s,str_p,str_o))\n",
    "        else:\n",
    "            if use_properties:\n",
    "                if str_s not in properties_and_types.keys():\n",
    "                    properties_and_types[str_s] =[]\n",
    "                p_type, p_value = get_property_type(str_o)\n",
    "                if (str_s,p_type, p_value) not in properties_and_types[str_s]:\n",
    "                    properties_and_types[str_s].append((get_type(str_p), p_type, p_value))\n",
    "                triples.append((str_s,str_p,str_o))\n",
    "    else:\n",
    "        if str_s not in entities_and_type.keys():\n",
    "            entities_and_type[str_s] =[]\n",
    "        triples.append((str_s,str_p,str_o))\n",
    "        split_o = str_o.split('/')\n",
    "        entities_and_type[str_s].append(split_o[len(split_o)-1])\n",
    "\n",
    "for e in entities_and_type:\n",
    "    entities_and_type[e].sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_types(subj_type, obj_type):\n",
    "    if (subj_type,obj_type) not in possible_types:\n",
    "        \n",
    "        q = \"SELECT DISTINCT ?property WHERE {\"+\\\n",
    "        \"{ ?property rdfs:domain dbo:\"+subj_type+\". ?property rdfs:range dbo:\"+obj_type+\\\n",
    "        \" .} UNION {dbo:\"+subj_type +\" rdfs:subClassOf ?superclass. dbo:\"+obj_type +\" rdfs:subClassOf  ?superclass2 .\"+\\\n",
    "        \"  ?property rdfs:domain ?superclass . ?property rdfs:range ?superclass2 \"+\\\n",
    "        \"} }\"\n",
    "        \n",
    "        result = ontology.query(q)\n",
    "        results = []\n",
    "        for res in result:\n",
    "            results.append(str(res[0]))\n",
    "        \n",
    "        q2 = \"SELECT DISTINCT ?property WHERE {\"+\\\n",
    "        \"{dbo:\"+subj_type +\" rdfs:subClassOf ?superclass. \"+\\\n",
    "        \" ?property rdfs:domain ?superclass . ?property rdfs:range dbo:\"+obj_type+\\\n",
    "        \" .} UNION {dbo:\"+obj_type +\" rdfs:subClassOf  ?superclass2 . ?property rdfs:domain dbo:\"+\\\n",
    "        subj_type+\" . ?property rdfs:range ?superclass2}}\"\n",
    "        \n",
    "        result = ontology.query(q2)\n",
    "\n",
    "        for res in result:\n",
    "            results.append(str(res[0]))\n",
    "\n",
    "        possible_types[(subj_type,obj_type)] = results\n",
    "        return results\n",
    "    return possible_types[(subj_type, obj_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_multiple_types(entities_and_type, s,p,o): \n",
    "    for subtype_subj in entities_and_type[str(s)]:\n",
    "        if len(entities_and_type[str(o)]) > 1:\n",
    "            for subtype_obj in entities_and_type[str(o)]:\n",
    "                possible_rels = get_possible_types( subtype_subj, subtype_obj)\n",
    "\n",
    "                if len(possible_rels) == 0:\n",
    "                    continue   \n",
    "                \n",
    "                p = get_type(p)\n",
    "                for rel in possible_rels:\n",
    "                    if get_type(rel) == p:\n",
    "                        return (subtype_subj,subtype_obj)\n",
    "        else:\n",
    "            subtype_obj = entities_and_type[str(o)][0]\n",
    "            possible_rels = get_possible_types(subtype_subj, subtype_obj)\n",
    "            if len(possible_rels) == 0:\n",
    "                    continue\n",
    "            p = get_type(p)   \n",
    "            for rel in possible_rels:\n",
    "                if get_type(rel) == p:\n",
    "                    return (subtype_subj,  subtype_obj)\n",
    "        \n",
    "    return (\"\",\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_triples = []\n",
    "added_types = []\n",
    "\n",
    "for s,p,o in triples:\n",
    "    s1 = str(s)\n",
    "    p1 = str(p)\n",
    "    o1 = str(o)\n",
    "\n",
    "    if p != str(RDF.type):\n",
    "        #if s1 in list(properties_and_types.keys()):\n",
    "        if o1.find(\"^^\") != -1:\n",
    "            #x = properties_and_type[str(s)]\n",
    "            new_triples.append((s, p, o))\n",
    "        if (s1 in list(entities_and_type.keys()) and\n",
    "            o1 in list(entities_and_type.keys())):\n",
    "            #se è una relazione tra classi\n",
    "\n",
    "            #se il soggetto o l'oggetto ha più di un tipo \n",
    "            if len(entities_and_type[str(s)]) > 1 or len(entities_and_type[str(o)]) > 1 :\n",
    "                new_subj_type, new_obj_type = disambiguate_multiple_types(entities_and_type,s,p,o)\n",
    "                if((new_subj_type, new_obj_type) == (\"\",\"\") \n",
    "                    or new_subj_type == \"\" \n",
    "                    or new_obj_type ==\"\"):\n",
    "                    continue\n",
    "                \n",
    "                entities_and_type[str(s)] = [new_subj_type]\n",
    "                entities_and_type[str(o)] = [new_obj_type]\n",
    "\n",
    "                if s not in added_types:\n",
    "                    new_triples.append((s, str(RDF.type),prefix + new_subj_type))\n",
    "                    added_types.append(s)\n",
    "                if o not in added_types:\n",
    "                    new_triples.append((o,str(RDF.type),prefix + new_obj_type ))\n",
    "                    added_types.append(o)\n",
    "            else: \n",
    "                if s not in added_types:\n",
    "                    new_triples.append((s, str(RDF.type),prefix+entities_and_type[str(s)][0] ))\n",
    "                    added_types.append(s)\n",
    "                if o not in added_types and str(o).find(\"^^\") == -1:\n",
    "                    new_triples.append((o, str(RDF.type),prefix+entities_and_type[str(o)][0] ))\n",
    "                    added_types.append(o)\n",
    "            if(s,p,o) not in new_triples:\n",
    "                new_triples.append((s, p, o))\n",
    "    else:\n",
    "        if s not in added_types and len(entities_and_type[str(s)]) == 1: \n",
    "            #controllo solo s perché o è il tipo, verifico che non ci sia piu di\n",
    "            #un tipo altrimenti rimando l'aggiunta\n",
    "            new_triples.append((s, p, o))\n",
    "            added_types.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_properties_and_types = {}\n",
    "for s in list(properties_and_types.keys()):\n",
    "    for element in properties_and_types[s]:\n",
    "        s_class = entities_and_type[s]\n",
    "        if s not in new_properties_and_types:\n",
    "            new_properties_and_types[s] = []\n",
    "        new_properties_and_types[s].append((s_class[0], element[0], element[1], element[2]))\n",
    "\n",
    "properties_and_types = new_properties_and_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types_count = {}\n",
    "property_types_count = {}\n",
    "entities = []\n",
    "for entity in entities_and_type.keys():\n",
    "    tipo = entities_and_type[entity][0]\n",
    "    if tipo != \"\":\n",
    "        entity_types_count[tipo] = entity_types_count.get(tipo, 0)+1\n",
    "        entities.append(entity)\n",
    "\n",
    "for subj in properties_and_types.keys():\n",
    "    for class_name, prop_name, prop_type, prop_value in properties_and_types[subj]:\n",
    "        property_types_count[(class_name, subj, prop_name, prop_type)] = property_types_count.get((class_name, subj, prop_name,prop_type), 0)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_dict = {t:{'count': 0} for t in entity_types_count.keys()}\n",
    "\n",
    "for class_name, subject,rel, p_type in property_types_count.keys():\n",
    "    index_dict[p_type] = {'count':0}\n",
    "    if class_name not in index_dict.keys():\n",
    "        index_dict[class_name] = {'count':0}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_triples.sort()\n",
    " \n",
    "#questi due dizionari contengono gli stessi indici e soo stati\n",
    "#divisi per popolare più velocemente edge_index\n",
    "subject_dict = {}\n",
    "object_dict = {}\n",
    "for triple in new_triples:\n",
    "    s = str(triple[0])\n",
    "    p = str(triple[1])\n",
    "    o = str(triple[2])\n",
    "    type_triples = []\n",
    "    if p != str(RDF.type):\n",
    "        s_type = entities_and_type[s][0] \n",
    "        p_type = get_type(p)\n",
    "        \n",
    "        if o.find(\"^^\") == -1:\n",
    "            o_type = entities_and_type[o][0]\n",
    "        else: \n",
    "            o_type = get_property_type(o)[0]\n",
    "        type_triples.append((s_type,p_type, o_type))\n",
    "\n",
    "        for s_type,p_type,o_type in type_triples:\n",
    "            if(s_type != \"\" and o_type != \"\"):\n",
    "                key_t = (s_type, p_type, o_type)\n",
    "\n",
    "                if key_t not in subject_dict.keys():\n",
    "                    subject_dict[key_t] = []\n",
    "                    object_dict[key_t] = []\n",
    "\n",
    "                if str(s) not in index_dict[s_type]:\n",
    "                    index_dict[s_type][str(s)] = index_dict[s_type]['count']\n",
    "                    index_dict[s_type]['count'] = index_dict[s_type]['count']+1\n",
    "                s_index = index_dict[s_type][str(s)]\n",
    "                \n",
    "       \n",
    "                if str(o) not in index_dict[o_type]:\n",
    "                    index_dict[o_type][str(o)] = index_dict[o_type]['count']\n",
    "                    index_dict[o_type]['count'] = index_dict[o_type]['count']+1\n",
    "                o_index = index_dict[o_type][str(o)]\n",
    "                \n",
    "                subject_dict[key_t].append(s_index)\n",
    "                object_dict[key_t].append(o_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from dateutil.parser import parse\n",
    "import datetime, string\n",
    "\n",
    "def function_build_feature(p_type, value):\n",
    "    #return [5] così funziona perchè è numerico\n",
    "    count = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "\n",
    "    #aggiungere funzione x riconscere le date\n",
    "    if p_type == 'Integer':\n",
    "        try: i = int(value) \n",
    "        except: i = 0\n",
    "        return [i]\n",
    "    if p_type == 'Double':\n",
    "        try: d = float(value)\n",
    "        except: d = float(0.0)\n",
    "        return [d]\n",
    "    if p_type == 'gYear':\n",
    "        return [int(1970-value)]\n",
    "    if p_type == 'String':\n",
    "        a_punct = count(value, string.punctuation)\n",
    "        lang = 0\n",
    "        try:\n",
    "            if detect(value) == 'en': lang = 1\n",
    "        except:\n",
    "            lang = 0\n",
    "        return [len(value), value.count(\" \") , value.count(\"(\") + value.count(\")\"), lang, a_punct]\n",
    "    if p_type == 'Date':\n",
    "        return [(parse(value) - datetime.datetime(1970,1,1)).days]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = HeteroData()\n",
    "\n",
    "data_to_insert = {}\n",
    "for subj in list(properties_and_types.keys()):\n",
    "    for class_type, prop_name, prop_type, prop_value in properties_and_types[subj]:\n",
    "        if prop_type not in data_to_insert:\n",
    "            data_to_insert[prop_type] = []\n",
    "        \n",
    "        p_count = property_types_count[(class_type, subj, prop_name, prop_type)]\n",
    "        for i in range(p_count):\n",
    "            data_to_insert[prop_type].append(function_build_feature(prop_type, prop_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(entity_types_count.keys())\n",
    "for t in types:\n",
    "    data_to_insert[t] = [[1] for i in range(entity_types_count[t])]\n",
    "\n",
    "for key in data_to_insert.keys():\n",
    "    lists = data_to_insert[key]\n",
    "    if lists != '':\n",
    "        complete_data[key].x = torch.Tensor(lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in subject_dict.keys(): #le keys di subject_dict ed object_dict sono ==\n",
    "    #ci sono 2 dizionari diversi per evitare il doppio for (complessità n^2)\n",
    "    lol = [subject_dict[triple], object_dict[triple]]\n",
    "    complete_data[triple[0], triple[1], triple[2]].edge_index = torch.Tensor(lol).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in complete_data.edge_index_dict.items():\n",
    "    max_ind_sx = int(max(v[1]))\n",
    "    try:\n",
    "        n1 = complete_data[k[2]].x[max_ind_sx]\n",
    "    except IndexError:\n",
    "        print(\"Relation:\", k, \" node type:\", k[2], \" index:\", max_ind_sx, f\"{k[2]} matrix dimension:\", len(complete_data[k[2]].x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_types = list(complete_data.edge_index_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import remove_self_loops, remove_isolated_nodes\n",
    "\n",
    "for edge_type in complete_data.edge_index_dict.keys():\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        new_edge_index = remove_self_loops(complete_data[edge_type].edge_index)[0]\n",
    "        complete_data[edge_type].edge_index = new_edge_index\n",
    "    new_edge_index = remove_isolated_nodes(complete_data[edge_type].edge_index)[0]\n",
    "    complete_data[edge_type].edge_index = new_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "link_split = RandomLinkSplit(num_val=0.0,\n",
    "                             num_test=0.25,\n",
    "                             edge_types=edge_types,\n",
    "                             rev_edge_types=[None]*len(edge_types))\n",
    "train_link, val_link, test_link = link_split(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero, GATConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        #Sto definendo quale tipologia di layer voglio usare.\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = GATConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #descrive la computazione dall'input all'output.\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GNN(hidden_channels=4, out_channels=2)\n",
    "model = to_hetero(model, complete_data.metadata(), aggr='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay=5e-4\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "criterion =  torch.nn.BCEWithLogitsLoss() #change loss function\n",
    "def train_hetlinkpre():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(train_link.x_dict, train_link.edge_index_dict)  # Perform a single forward pass.\n",
    "    preds = torch.Tensor()\n",
    "    edge_labels = torch.Tensor()\n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    for edge_t in edge_types:\n",
    "        #Compute link embedding for each edge type\n",
    "        #for src in train_link[edge_t].edge_label_index[0]:\n",
    "        out_src = out[edge_t[0]][train_link[edge_t].edge_label_index[0]]#embedding src nodes\n",
    "        out_dst = out[edge_t[2]][train_link[edge_t].edge_label_index[1]] #embedding dst nodes\n",
    "        \n",
    "        # LINK EMBEDDING #\n",
    "        # 1 - Dot Product\n",
    "        out_sim = out_src * out_dst #dotproduct\n",
    "        pred = torch.sum(out_sim, dim=-1)\n",
    "        \n",
    "        preds = torch.cat((preds,pred),-1)\n",
    "        edge_labels = torch.cat((edge_labels,train_link[edge_t].edge_label.type_as(pred)),-1)\n",
    "    \n",
    "        \n",
    "    #compute loss function based on all edge types\n",
    "    loss = criterion(preds, edge_labels)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(complete_data.x_dict,complete_data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_hetlinkpre(test_link):\n",
    "    model.eval()\n",
    "    out = model(test_link.x_dict, test_link.edge_index_dict)\n",
    "    \n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    \n",
    "    hs = torch.Tensor()\n",
    "    edge_labels = np.array([])\n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    for edge_t in edge_types:\n",
    "        #Compute link embedding for each edge type\n",
    "        #for src in train_link[edge_t].edge_label_index[0]:\n",
    "        out_src = out[edge_t[0]][test_link[edge_t].edge_label_index[0]]#embedding src nodes\n",
    "        out_dst = out[edge_t[2]][test_link[edge_t].edge_label_index[1]] #embedding dst nodes\n",
    "        \n",
    "        # LINK EMBEDDING #\n",
    "        # 1 - Dot Product\n",
    "        out_sim = out_src * out_dst #dotproduct\n",
    "        h = torch.sum(out_sim, dim=-1)\n",
    "        hs = torch.cat((hs,h),-1)\n",
    "        edge_labels = np.concatenate((edge_labels,test_link[edge_t].edge_label.cpu().detach().numpy()))\n",
    "    \n",
    "    \n",
    "    pred_cont = torch.sigmoid(hs).cpu().detach().numpy()\n",
    "    \n",
    "    # EVALUATION\n",
    "    test_roc_score = roc_auc_score(edge_labels, pred_cont) #comput AUROC score for test set\n",
    "    \n",
    "    return test_roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_train = []\n",
    "perf_test = []\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_hetlinkpre()\n",
    "    roc_train = test_hetlinkpre(train_link)\n",
    "    roc_test = test_hetlinkpre(test_link)\n",
    "    perf_train.append(roc_train)\n",
    "    perf_test.append(roc_test)\n",
    "    #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUROC: 0.9451\n",
      "Test AUROC: 0.8779\n"
     ]
    }
   ],
   "source": [
    "roc_train = test_hetlinkpre(train_link)\n",
    "roc_test = test_hetlinkpre(test_link)\n",
    "print(f'Train AUROC: {roc_train:.4f}\\nTest AUROC: {roc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hetscores(test_link):\n",
    "    model.eval()\n",
    "    out = model(test_link.x_dict, test_link.edge_index_dict)\n",
    "    \n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    \n",
    "    hs = torch.Tensor()\n",
    "    ### LINK PREDICTION ACTS HERE ###\n",
    "    for edge_t in test_link.edge_index_dict.keys():\n",
    "        #Compute link embedding for each edge type\n",
    "        #for src in train_link[edge_t].edge_label_index[0]:\n",
    "        out_src = out[edge_t[0]][test_link[edge_t].edge_index[0]]#embedding src nodes\n",
    "        out_dst = out[edge_t[2]][test_link[edge_t].edge_index[1]] #embedding dst nodes\n",
    "        \n",
    "        # LINK EMBEDDING #\n",
    "        # 1 - Dot Product\n",
    "        out_sim = out_src * out_dst #dotproduct\n",
    "        h = torch.sum(out_sim, dim=-1)\n",
    "        \n",
    "        hs = torch.cat((hs,h),-1)\n",
    "    \n",
    "    \n",
    "    pred_cont = torch.sigmoid(hs).cpu().detach().numpy()\n",
    "    \n",
    "    return pred_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = HeteroData()\n",
    "for triple in complete_data.edge_index_dict.keys():\n",
    "    test_data[triple].edge_index = torch.Tensor([[],[]]).long()\n",
    "    test_data[triple[0]].x = torch.Tensor([[1]])\n",
    "    if test_data[triple[2]] == 'String':\n",
    "        test_data[triple[2]].x = torch.Tensor([[1,1,1,1,1]])\n",
    "    else:\n",
    "        test_data[triple[2]].x = torch.Tensor([[1]])\n",
    "\n",
    "test_data['Person'].x = torch.Tensor([[1]])\n",
    "test_data['City'].x = torch.Tensor([[1]])\n",
    "test_data['Person', 'bornIn', 'City'].edge_index = torch.Tensor([[0],[0]]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mActor\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mFilm\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mDate\u001b[0m={},\n",
       "  \u001b[1mString\u001b[0m={},\n",
       "  \u001b[1mDirector\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mInteger\u001b[0m={},\n",
       "  \u001b[1mPerson\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mProductionCompany\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mDouble\u001b[0m={},\n",
       "  \u001b[1mCreativeWork\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mCity\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mCountry\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mTelevisionShow\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1mLocation\u001b[0m={ x=[1, 1] },\n",
       "  \u001b[1m(Actor, starring, Film)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, birthDate, Date)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, birthName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, directedBy, Director)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, children, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, cinematography, Person)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, tradingName, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, budget, Double)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, birthName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(CreativeWork, title, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(CreativeWork, genre, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, title, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, producedBy, ProductionCompany)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, cityName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, foundedBy, Director)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, bornIn, Country)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, language, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, birthDate, Date)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, almaMater, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, bornIn, City)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, bornIn, City)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Country, countryName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, starring, TelevisionShow)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, bornIn, Country)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, birthName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, children, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, title, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, spouse, Actor)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, headquarter, City)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, createdBy, Director)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, birthDate, Date)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, postalCode, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, broadcaster, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, genre, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, foundedBy, Actor)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, language, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, foundingYear, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, bornIn, City)\u001b[0m={ edge_index=[2, 1] },\n",
       "  \u001b[1m(Actor, title, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Country, languageCountry, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, education, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, channel, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, awards, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, children, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, children, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, knownFor, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, bornIn, Country)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, cinematography, Actor)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, tradingName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, children, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Country, population, Date)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, area, Double)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, keyPerson, Actor)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, in, Country)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, knownFor, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, headquarter, Country)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, editing, Person)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(CreativeWork, takesPlaceIn, Location)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, postalCode, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, author, TelevisionShow)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Country, capital, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, spouse, Director)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, numSeasons, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(City, languageCountry, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, author, CreativeWork)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(TelevisionShow, numEpisodes, Integer)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Person, spouse, Person)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Actor, bornIn, Location)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, keyPerson, Person)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Location, cityName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, starring, TelevisionShow)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Country, cityName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, keyPerson, Director)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Film, budget, Date)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(Director, tradingName, String)\u001b[0m={ edge_index=[2, 0] },\n",
       "  \u001b[1m(ProductionCompany, foundedBy, Person)\u001b[0m={ edge_index=[2, 0] }\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hetscores(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18136/308060903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_hetscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{triple}: {weight}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18136/2492703810.py\u001b[0m in \u001b[0;36mtest_hetscores\u001b[0;34m(test_link)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_hetscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### LINK PREDICTION ACTS HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fase2/HeteroGraph/lib/python3.7/site-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mwrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     print(generate_error_message(topmost_framesummary),\n\u001b[1;32m    615\u001b[0m                           file=sys.stderr)\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)"
     ]
    }
   ],
   "source": [
    "test_data = HeteroData()\n",
    "for triple in complete_data.edge_index_dict.keys():\n",
    "    for triple2 in complete_data.edge_index_dict.keys():\n",
    "        test_data[triple2].edge_index = torch.Tensor([[],[]]).long()\n",
    "        test_data[triple2[0]].x = torch.Tensor([[1]])\n",
    "        if test_data[triple2[2]] == 'String':\n",
    "            test_data[triple2[2]].x = torch.Tensor([[1, 1, 1, 1, 1]])\n",
    "    test_data[triple[0]].x = torch.Tensor([[1]])\n",
    "    \n",
    "    if test_data[triple[2]] == 'String':\n",
    "        test_data[triple[2]].x = torch.Tensor([[1]])\n",
    "    test_data[triple].edge_index = torch.Tensor([[0],[0]]).long()\n",
    "    weight = test_hetscores(test_data)[0]\n",
    "    print(f'{triple}: {weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18136/1094291145.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_hetscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18136/2492703810.py\u001b[0m in \u001b[0;36mtest_hetscores\u001b[0;34m(test_link)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_hetscores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_link\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### LINK PREDICTION ACTS HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fase2/HeteroGraph/lib/python3.7/site-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mwrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     print(generate_error_message(topmost_framesummary),\n\u001b[1;32m    615\u001b[0m                           file=sys.stderr)\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18136/2350515931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Initialize lazy modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/fase2/HeteroGraph/lib/python3.7/site-packages/torch/fx/graph_module.py\u001b[0m in \u001b[0;36mwrapped_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     print(generate_error_message(topmost_framesummary),\n\u001b[1;32m    615\u001b[0m                           file=sys.stderr)\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 5x4)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('HeteroGraph': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed8a8f81fdcbe4bb580e703d3d81905dffa90d7a53be09b673d3f1a6169b3bc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
